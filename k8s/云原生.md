

# **cloud native camp**

```markdown
### go语言基础

[<https://draveness.me/golang/>](<https://draveness.me/golang/>)

### 集群安装

[<https://github.com/cncamp/101>](<https://github.com/cncamp/101>)

- readme中列举了三种办法, minikube, kubead, kind
- 对于机器配置不够好的同学，建议试一下kind
- 对于机器性能不错的同学，建议用kubeadm

另外101中有一些简单的教程，比如比如understand docker，microservice之类的，是一些命令，大家可以试试看效果

### linux内核代码处理网络包的过程
[<https://pouncing-waterfall-7c4.notion.site/network-stack-35b42c1a77834de18c1ecd510d1e0d1b>](<https://www.notion.so/network-stack-35b42c1a77834de18c1ecd510d1e0d1b>)

### Kubernetes informer框架代码分析

[<https://pouncing-waterfall-7c4.notion.site/informer-framework-31ba746049ec472fb405e61482ed762f>](<https://www.notion.so/informer-framework-31ba746049ec472fb405e61482ed762f>)

### kubelet podWorker启动pod代码细节
[<https://pouncing-waterfall-7c4.notion.site/kubelet-go-c3b5cf9bbf4b4e3098720f61efb15e0e>](<https://www.notion.so/kubelet-go-c3b5cf9bbf4b4e3098720f61efb15e0e>)

### istio组件交互概览

[<https://pouncing-waterfall-7c4.notion.site/Istio-component-interactions-b6e8c654b71544e98598d14282f3d940>](<https://www.notion.so/Istio-component-interactions-b6e8c654b71544e98598d14282f3d940>)

### istiod 初始化细节代码
[<https://pouncing-waterfall-7c4.notion.site/Istiod-start-flow-ba1cfac7b8a64921b4204ae3d3c101e8>](<https://www.notion.so/Istiod-start-flow-ba1cfac7b8a64921b4204ae3d3c101e8>)
```



## linux内核代码处理网络包的过程

**network stack**

dev.c

net dev init to register function for softirq and cpu poll function

```c
net_dev_init()-->
	sd->backlog.poll = process_backlog
	open_softirq(NET_TX_SOFTIRQ, net_tx_action);
	open_softirq(NET_RX_SOFTIRQ, net_rx_action)
```

hardware irq handler, it only trigger softirq

```c
igb_main.c
__igb_open()-->igb_request_irq()-->igb_request_msix()-->igb_msix_ring()-->

igb_msix_ring(int irq, void *data)-->
	igb_write_itr(q_vector)//count irq
	napi_schedule(&q_vector->napi)-->dev.c:__napi_schedule()-->
		list_add_tail(&napi->poll_list, &sd->poll_list) // add napi poll list to sd poll list
		__raise_softirq_irqoff(NET_RX_SOFTIRQ)-->
			trace_softirq_raise(nr)
```

net_rx_action is softirq handler , it calles tc and then packet_type→func which is registered as ip_rcv

```c
net_rx_action(struct softirq_action *h)-->
	napi_poll(n, &repoll)-->
		n->poll(n, weight)-->process_backlog-->
			skb = __skb_dequeue(&sd->process_queue)
			__netif_receive_skb(skb)-->
				__netif_receive_skb_one_core(skb, false)-->
					__netif_receive_skb_core(skb, pfmemalloc, &pt_prev)-->
						skb_reset_network_header(skb)
						skb_reset_mac_len(skb)
						deliver_skb(skb, pt_prev, orig_dev)->
							//packet_type->func
							pt_prev->func(skb, skb->dev, pt_prev, orig_dev)--> 
				// tc logic goes here
				skip_taps:
					sch_handle_ingress()-->
						tcf_classify()-->
```

ip_rcv handle the logic of receiving an ip packet, it finally calls skb_dst→input which is registered as ip_local_deliver

```c
static struct packet_type ip_packet_type __read_mostly = {
	.type = cpu_to_be16(ETH_P_IP),
	.func = ip_rcv,
	.list_func = ip_list_rcv,
};

ip_input.c
ip_rcv()-->
	ip_rcv_core(skb, net)-->
		ip_hdr(skb)
		NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, net, NULL, skb, dev, NULL, ip_rcv_finish)-->ip_rcv_finish-->ip_rcv_finish_core-->
			ip_route_input_noref(skb, iph->daddr, iph->saddr, iph->tos, dev)-->
				ip_route_input_rcu()-->
					ip_route_input_slow(skb, daddr, saddr, tos, dev, res)-->//find dst from fib(routing table) and set dst
						rth = rt_dst_alloc(l3mdev_master_dev_rcu(dev) ? : net->loopback_dev, flags | RTCF_LOCAL, res->type, IN_DEV_CONF_GET(in_dev, NOPOLICY), false, do_cache)-->
							rt->dst.input = ip_local_deliver
			dst_input(skb)-->
				skb_dst(skb)->input(skb) //this will trigger ip_local_deliver
```

ip_local_deliver invoke net filter hook for LOCAL_IN , and then call ip_local_deliver_finish as the complete function, which retrieve higher level protocol from the packet IP header, the protocol is TCP/UDP/ICMP, etc, and then it call the handler function of corresponding upper level protocols

```c
ip_local_deliver
					return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN,
		       net, NULL, skb, skb->dev, NULL,
		       ip_local_deliver_finish)-->
						ip_protocol_deliver_rcu(net, skb, ip_hdr(skb)->protocol)-->
							ipprot = rcu_dereference(inet_protos[protocol])//tcp/udp/icmp
							ipprot->handler(skb)// To upper protocol
```

the following code register handler for tcp protocol

```c
/* thinking of making this const? Don't.
 * early_demux can change based on sysctl.
 */
static struct net_protocol tcp_protocol = {
	.early_demux	=	tcp_v4_early_demux,
	.early_demux_handler =  tcp_v4_early_demux,
	.handler	=	tcp_v4_rcv,
	.err_handler	=	tcp_v4_err,
	.no_policy	=	1,
	.netns_ok	=	1,
	.icmp_strict_tag_validation = 1,
};
```

tcp handler will handle data packet transfer in tcp layer, the detail is not listed here, but basically it look for the socket info, retrieve the data from tcp receive queue and put it to socket receive queue, and wait for app to read them

```c
tcp_v4_rcv()-->
	struct sock *sk;
	tcp_v4_do_rcv-->
	lookup:
		sk = __inet_lookup_skb(&tcp_hashinfo, skb, __tcp_hdrlen(th), th->source,
				       th->dest, sdif, &refcounted);
		if (!sk)
			goto no_tcp_socket;
		if (sk->sk_state == TCP_LISTEN) 
			ret = tcp_v4_do_rcv(sk, skb)-->
				tcp_rcv_established(sk, skb)-->
					struct tcp_sock *tp = tcp_sk(sk)
					__skb_pull(skb, tcp_header_len)
					tcp_queue_rcv()-->
						__skb_queue_tail(&sk->sk_receive_queue, skb)
```



## Kubernetes informer框架代码分析

**informer framework**

Generated code

```go
secretInformer := kubecoreinformers.NewSecretInformer()-->
	NewFilteredSecretInformer()-->
		NewSharedIndexInformer(&cache.**ListWatch**{}, &corev1.Secret{}, resyncPeriod, indexers)-->
			sharedIndexInformer := &sharedIndexInformer{
				processor:                       &sharedProcessor{clock: realClock},
				indexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),
				listerWatcher:                   lw,
				objectType:                      exampleObject,
				resyncCheckPeriod:               defaultEventHandlerResyncPeriod,
				defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod,
				cacheMutationDetector:           NewCacheMutationDetector(fmt.Sprintf("%T", exampleObject)),
				clock:                           realClock,
			}

secretInformer.AddEventHandler()-->
	AddEventHandlerWithResyncPeriod()-->
		listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize)-->
			ret := &processorListener{
				nextCh:                make(chan interface{}),
				addCh:                 make(chan interface{}),
				handler:               handler,
				pendingNotifications:  *buffer.NewRingGrowing(bufferSize),
				requestedResyncPeriod: requestedResyncPeriod,
				resyncPeriod:          resyncPeriod,
			}
		s.processor.addListener(listener)
			listener.run()-->
				for **next := range p.nextCh** {
					p.handler.OnUpdate(notification.oldObj, notification.newObj)
					p.handler.OnAdd(notification.newObj)
					p.handler.OnDelete(notification.oldObj)
				}
			listener.pop()-->
				for {
					select {
					case nextCh <- notification:
						notification, ok = p.pendingNotifications.ReadOne()
					case notificationToAdd, ok := <-p.addCh:
							p.pendingNotifications.WriteOne(notificationToAdd)
				}
		for _, item := range s.indexer.List() {
			listener.add(addNotification{newObj: item})-->
				**p.addCh <- notification**
		}

go secretInformer.Run(ctx.Stop)
	fifo := NewDeltaFIFOWithOptions()
	cfg := &Config{
			Queue:            fifo,
			ListerWatcher:    s.listerWatcher,
			ObjectType:       s.objectType,
			FullResyncPeriod: s.resyncCheckPeriod,
			RetryOnError:     false,
			ShouldResync:     s.processor.shouldResync,
	
			Process: s.HandleDeltas,
		}
	wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)
	wg.StartWithChannel(processorStopCh, s.processor.run)
	s.controller = New(cfg)
	s.controller.Run(stopCh)-->
		r := NewReflector(
			c.config.ListerWatcher,
			c.config.ObjectType,
			c.config.Queue,
			c.config.FullResyncPeriod,
		)
		wg.StartWithChannel(stopCh, r.Run)-->
			r.ListAndWatch(stopCh)-->
				**list := pager.List(context.Background(), options) (1)**
				items, err := meta.ExtractList(list)
				r.syncWith(items, resourceVersion)-->
					**r.store.Replace(found, resourceVersion) (2)**
				r.watchHandler(start, w, &resourceVersion, resyncerrc, stopCh)-->
					**r.store.Update(event.Object)**
		c.processLoop-->
			c.config.Queue.Pop(PopProcessFunc(c.config.Process))//HandleDeltas
				for _, d := range obj.(Deltas) {
					s.processor.distribute(updateNotification)
					s.processor.distribute(addNotification)
					s.processor.distribute(deleteNotification)
				}
```



## kubelet podWorker启动pod代码细节

**kubelet.go**

```go
server.go:NewKubeletCommand()-->>
kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)-->>
	plugins, err := ProbeVolumePlugins(featureGate)
server.go:Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh)-->>
	run(s, kubeDeps, featureGate, stopCh)-->>
		kubeDeps.ContainerManager, err = cm.NewContainerManager() // init runtime service(CRI), -container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///var/containerd/containerd.sock
		kubelet.PreInitRuntimeService()-->>
			remote.NewRemoteRuntimeService(remoteRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration)
		RunKubelet(s, kubeDeps, s.RunOnce)-->>
			createAndInitKubelet()-->>
				kubelet.NewMainKubelet()-->>
					makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)-->>
						updatechannel = cfg.Channel(kubetypes.ApiserverSource)
					klet := &Kubelet{}
					//*******init volume plugins
					runtime, err := kuberuntime.NewKubeGenericRuntimeManager()
					NewInitializedVolumePluginMgr()-->>
						kvh.volumePluginMgr.InitPlugins(plugins, prober, kvh)-->>
			startKubelet()-->>
				k.Run(podCfg.Updates())-->>
					//*******run volume manager, and reconcile function would attach volume for attachable plugin and mount volume
					go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)-->>
						vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh)-->>
							populatorLoop-->>
								dswp.findAndAddNewPods()-->>
									dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize)-->>
										mounts, devices := util.GetPodVolumeNames(pod)
										dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mounts, devices)
										dswp.desiredStateOfWorld.AddPodToVolume(uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue)-->>
											dsw.volumesToMount[volumeName] = volumeToMount{}
						vm.reconciler.Run(stopCh)-->>	
							reconciliationLoopFunc() -->> // reconcile every 100 ms
								mountAttachVolumes-->>
									rc.desiredStateOfWorld.GetVolumesToMount()
									rc.operationExecutor.AttachVolume()-->>	// attachable plugin, e.g. 	CSI plugin
										operationGenerator.GenerateAttachVolumeFunc(volumeToAttach, actualStateOfWorld).Run()
									rc.operationExecutor.MountVolume()-->>  // volume need to mount, like ceph, configmap, emptyDir
										oe.operationGenerator.GenerateMapVolumeFunc().Run()-->>
											volumePlugin, err := og.volumePluginMgr.FindPluginBySpec(volumeToMount.VolumeSpec)
											volumePlugin.NewMounter()
											volumeMounter.SetUp()
					kl.syncLoop(updates, kl)-->>
						kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh)-->>
							//*******handle pod creation event
							handler.HandlePodAdditions(u.Pods)-->>
								kl.podManager.AddPod(pod)
								kl.canAdmitPod(activePods, pod) // check admit, if admit check fail, it will error out
								kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)-->>
									kl.podWorkers.UpdatePod()-->>
										p.managePodLoop(podUpdates)-->>
											p.syncPodFn()-->>
												kubelet.go:syncPod()-->>
													runnable := kl.canRunPod(pod) // check soft admin, pod will be pending if check fails
													kl.runtimeState.networkErrors() // check network plugin status
													kl.containerManager.UpdateQOSCgroups()
													kl.makePodDataDirs(pod)
													kl.volumeManager.WaitForAttachAndMount(pod)
													(kubeGenericRuntimeManager)kl.containerRuntime.SyncPod()-->>
														m.computePodActions(pod, podStatus) // create sandbox container?
														m.createPodSandbox(pod, podContainerChanges.Attempt)-->>
															m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755)
															//*******calling CRI
															m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler)-->>// k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go
																c.cc.Invoke(ctx, "/runtime.v1alpha2.RuntimeService/RunPodSandbox") // call remote runtime service which is served by containerd
															start("init container", containerStartSpec(container))-->>
																startContainer()
															start("container", containerStartSpec(&pod.Spec.Containers[idx]))-->>
																startContainer()-->>
																	m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig)-->>
																		c.cc.Invoke(ctx, "/runtime.v1alpha2.ImageService/PullImage"
																	m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)-->>
																		c.cc.Invoke(ctx, "/runtime.v1alpha2.RuntimeService/CreateContainer")
																	m.internalLifecycle.PreStartContainer(pod, container, containerID) // set cpu set
																	m.runtimeService.StartContainer(containerID)-->>
																		c.cc.Invoke(ctx, "/runtime.v1alpha2.RuntimeService/StartContainer")
															
containerd
server.go:service.Register(grpcServer)-->>
	runtime.RegisterRuntimeServiceServer(s, instrumented)-->>
			s.RegisterService(&_RuntimeService_serviceDesc, srv)-->>
				Handler:    _RuntimeService_RunPodSandbox_Handler,	// api.pb.go
				srv.(RuntimeServiceServer).RunPodSandbox(ctx, in)-->> //"/runtime.v1alpha2.RuntimeService/RunPodSandbox"
					RunPodSandbox()-->> // pkg/server/sandbox_run.go
						sandboxstore.NewSandbox()
						c.ensureImageExists()
						c.getSandboxRuntime(config, r.GetRuntimeHandler())
						netns.NewNetNS()
						c.setupPodNetwork(ctx, &sandbox)-->>
							c.netPlugin.Setup(ctx, id, path, opts...)-->>
								network.Attach(ctx, ns)-->>
									n.cni.AddNetworkList(ctx, n.config, ns.config(n.ifName))-->>
										c.addNetwork(ctx, list.Name, list.CNIVersion, net, result, rt)-->>// for each network plugin
											c.exec.FindInPath(net.Network.Type, c.Path)
											buildOneConfig(name, cniVersion, net, prevResult, rt)
											invoke.ExecPluginWithResult(ctx, pluginPath, newConf.Bytes, c.args("ADD", rt), c.exec)
						c.client.NewContainer(ctx, id, opts...)
						c.os.MkdirAll(sandboxRootDir, 0755)
						c.os.MkdirAll(volatileSandboxRootDir, 0755)
						c.setupSandboxFiles(id, config)
						container.NewTask(ctx, containerdio.NullIO, taskOpts...)
						task.Start(ctx)-->>
							c.client.TaskService().Create(ctx, request)-->>
								s.local.Create(ctx, r)-->>
									l.getRuntime(container.Runtime.Name)
									rtime.Create(ctx, r.ContainerID, opts)-->>
										b := shimBinary(ctx, bundle, opts.Runtime, m.containerdAddress, m.containerdTTRPCAddress, m.events, m.tasks)
										b.Start()
										shim.Create(ctx, opts)-->>
											c.client.Call(ctx, "containerd.task.v2.Task", "Create", req, &resp)

	runtime.RegisterImageServiceServer(s, instrumented)
```



## istio组件交互概览

**Istio component interactions**

![img](D:\学习资料\笔记\k8s\k8s图\Istio component interactions)



- Kubernetes Informers to watch object changes and computes ConfigsUpdated, and put to global pushChannel
- Debounce function to run by interval, merge request and build Connection based PushQueue(conn as the queue element)
- XDSServer.doSendPush dequeue PushQueue and put env to client based pushChannel
- ADSServer establish connection to peer, Dequeue element from pushChannel, generate Envoy config by cds→eds→lds→rds→sds(defined by PushOrder) and send to Envoy



## istiod 初始化细节代码

**Istiod start flow** 

**NewServer() //pilot/pkg/bootstrap/server.go**

```go
e := &model.Environment{}
ac := aggregate.NewController(aggregate.Options{
	MeshHolder: e,
})
e.ServiceDiscovery = ac
s := &Server{}
e.TrustBundle = s.workloadTrustBundle
s.XDSServer = xds.NewDiscoveryServer()
s.initKubeClient() // TODO: client side rate limit is set here
s.initMeshConfiguration()
s.initMeshNetworks(args, s.fileWatcher)
s.initMeshHandlers()
s.environment.Init()
[s.initControllers()](<https://www.notion.so/s-initControllers-791515dfbbf94b5baf449e5bb6a7e261>)
```

discoveryServer.Start(stop)

```go
s.server.Start(stop)
```



**[s.initControllers()](https://www.notion.so/s-initControllers-791515dfbbf94b5baf449e5bb6a7e261)**

// Provision and manage the certificates for non-Pilot services.

```go
s.initCertController(args)
[s.initConfigController(args)](<https://www.notion.so/s-initConfigController-args-6aeea6da20044480b20c62cc28be5d7b>)
[s.initServiceControllers(args)](<https://www.notion.so/s-initServiceControllers-args-197e780387344d28b15834c330abe48c>)
s.initWorkloadTrustBundle(args) // controlled by ISTIO_MULTIROOT_MESH
s.initIstiodCerts() // Save the certificates to ./var/run/secrets/istio-dns. This is a memory-mounted dir.
s.initSecureDiscoveryService()-->
	net.Listen("tcp", args.ServerOptions.SecureGRPCAddr)
	s.XDSServer.Register(s.secureGrpcServer) // register grpc handler
	reflection.Register(s.secureGrpcServer)
s.initSecureWebhookServer(args)
s.initSidecarInjector(args)
s.initConfigValidation(args)
s.initIstiodAdminServer(args, whc)
s.initRegistryEventHandlers()-->
	s.ServiceController().AppendServiceHandler(serviceHandler) // s.XDSServer.ConfigUpdate(pushReq)
	s.configController.RegisterEventHandler(schema.Resource().GroupVersionKind(), configHandler) //s.XDSServer.ConfigUpdate(pushReq)
// initDiscoveryService intializes discovery server on plain text port.
s.initDiscoveryService(args)
s.initSDSServer(args)-->
	kubesecrets.NewMulticluster(s.kubeClient, s.clusterID, args.RegistryOptions.ClusterRegistriesNamespace, make(chan struct{}))-->
		secretcontroller.StartSecretController()
// Start CA or RA server. This should be called after CA and Istiod certs have been created.
s.startCA(caOpts)
s.addReadinessProbe("discovery", func() (bool, error) {
	return s.XDSServer.IsServerReady(), nil
})
```



**s.initConfigController(args)**

```go
s.initStatusController()
	status.NewController() // TODO: status controller client rate limit are here
	controller.Start() // TODO: this function computes the status update ratio every 200 ms
s.initK8SConfigStore()
	// configController is a ConfigCacheStore
	configController := makeKubeConfigController()-->
		crdclient.New()-->
			// init istio client, istio object informer and reg handler
			// TODO: all istio obj and service apis share same queue
			NewForSchemas()-->
			// queue element is a function which obj and registered handler as parameter
			createCacheHandler()-->
				cl.queue.Push(func() error {
					return h.onEvent(nil, obj, model.EventAdd)
				})
	s.initInprocessAnalysisController(args)
	s.RWConfigStore, err = configaggregate.MakeWriteableCache(s.ConfigStores, configController)
	// NewController create a controller which manages workload lifecycle and health status.
	s.XDSServer.WorkloadEntryController = workloadentry.NewController()
	aggregateConfigController, err := configaggregate.MakeCache(s.ConfigStores)
	s.configController = aggregateConfigController
	s.environment.IstioConfigStore = model.MakeIstioStore(s.configController)
	// Defer starting the controller until after the service is created.
	s.addStartFunc(func(stop <-chan struct{}) error {
	   go s.configController.Run(stop)
	   return nil
	})
```

// handler function cache_handler.go:onEvent()-->

```go
//================================
// handler is registerred by server.go:initRegistryEventHandlers()
// which will call s.XDSServer.ConfigUpdate(pushReq)
	for _, f := range h.handlers {
		f(oldConfig, currConfig, event)
	}
//================================
```



**s.initServiceControllers(args)**

```go
// init ServiceEntryStore
// register handler for SE and WE
**serviceentry.NewServiceDiscovery()-->**
	ServiceEntryStore()
	configController.RegisterEventHandler(s.serviceEntryHandler)-->
		// serviceEntryHandler()
		// convert serviceEntry to services based on hosts and ports
		convertServices()
		addedSvcs, deletedSvcs, updatedSvcs, unchangedSvcs = servicesDiff(os, cs)
		s.XdsUpdater.SvcUpdate() // add/update/delete
		s.edsUpdate(instances, true)
		s.XdsUpdater.ConfigUpdate(pushReq)
	configController.RegisterEventHandler()-->
		convertWorkloadEntryToWorkloadInstance() // workloadInstance: addr+portMap
		h(si, event) -->
			// handler is registerred in pilot/pkg/serviceregistry/kube/controller/controller.go:WorkloadInstanceHandler
			// WorkloadInstanceHandler()
			// Allow selecting workload entries from k8s services (#23683)
			getPodServices() 
			c.xdsUpdater.EDSUpdate(c.clusterID, string(service.Hostname), service.Attributes.Namespace, endpoints)
		// compute delta and update map cache
		s.edsUpdate()
		s.XdsUpdater.ProxyUpdate()
		s.XdsUpdater.ConfigUpdate()
serviceControllers.AddRegistry(s.serviceEntryStore)
**s.initKubeRegistry()-->**
	// TODO: Resync time was configured to 0, resetting to 30
	kubecontroller.NewMulticluster()
	// initialize the "main" cluster
	mc.AddMemberCluster()-->
		kubeRegistry := [NewController(client, options)](<https://www.notion.so/NewController-client-options-8e772cfd71364ad99264321ec0b6d8b6>)-->
		m.serviceController.AddRegistry(kubeRegistry)
		kubeRegistry.AppendServiceHandler(func(svc *model.Service, ev model.Event) { m.updateHandler(svc) })-->
			m.XDSUpdater.ConfigUpdate(req)
		kubeRegistry.AppendWorkloadHandler(m.serviceEntryStore.WorkloadInstanceHandler)
		NewNamespaceController(m.fetchCaRoot, client)
		// Patch validation webhook cert
		go validationWebhookController.Start(clusterStopCh)
		kube.RunAndWait()-->
			c.kubeInformer.Start(stop)
			c.dynamicInformer.Start(stop)
			c.metadataInformer.Start(stop)
			c.istioInformer.Start(stop)
			c.gatewayapiInformer.Start(stop)
	// start remote cluster controllers
	s.addStartFunc(func(stop <-chan struct{}) error {
		mc.InitSecretController(stop)
		return nil
	})
	s.multicluster = mc
```



**NewController(client, options)**

```go
filter.NewDiscoveryNamespacesFilter(c.nsInformer.Lister(), options.MeshWatcher.Mesh().DiscoverySelectors)
// handle discovery namespace membership changes triggered by namespace events,
c.initDiscoveryHandlers(kubeClient, options.EndpointMode, options.MeshWatcher, c.discoveryNamespacesFilter)-->
	c.initDiscoveryNamespaceHandlers(kubeClient, endpointMode, discoveryNamespacesFilter)-->
		c.handleSelectedNamespace()-->
			// list each service/pod/ep/epslice in the namespace and process add event
			c.onServiceEvent(svc, model.EventAdd)
			c.pods.onEvent(pod, model.EventAdd)
			c.endpoints.onEvent(ep, model.EventAdd)
	// handle discovery namespace membership changes triggered by changes to meshConfig's discovery selectors
	c.initMeshWatcherHandler(kubeClient, endpointMode, meshWatcher, discoveryNamespacesFilter)
**registerHandlers(c.serviceInformer, c.queue, "Services", c.onServiceEvent, nil)-->**
	q.Push()
//============= onServiceEvent is handler for service event 
onServiceEvent()-->
	kube.ConvertService()// convert k8s service, persist lbAddr
	needsFullPush := extractGatewaysFromService()// extract gateway for LB type service which selector gateways, TODO: need to check performance impact when there are thousands of gw services
		extractGatewaysInner()-->
			getGatewayDetails()// get cross network port(networking.istio.io/gatewayPort 15443) and network(topology.istio.io/network), return empty result when network is not specified
			if gwPort == 0 || network == "" {return false} // important, if has such settings, service change will trigger fullPush
			if gwPort changed or network changed {return true}
	if needsFullPush {
		c.xdsUpdater.ConfigUpdate(&model.PushRequest{Full: true, Reason: []model.TriggerReason{model.NetworksTrigger}})
	}
	c.endpoints.buildIstioEndpointsWithService(svc.Name, svc.Namespace, svcConv.Hostname)-->
		buildIstioEndpoints()--》
			NewEndpointBuilder(e.c, pod)
			builder.buildIstioEndpoint(ea.IP, port.Port, port.Name)
	c.xdsUpdater.EDSCacheUpdate(c.clusterID, string(svcConv.Hostname), svc.Namespace, endpoints)
	c.xdsUpdater.SvcUpdate(c.clusterID, string(svcConv.Hostname), svc.Namespace, event)
	// Notify service handlers.
	for _, f := range c.serviceHandlers {
		f(svcConv, event)// m.XDSUpdater.ConfigUpdate(req)
	}
//==============
newEndpointsController()-->
	**registerHandlers(informer, c.queue, "Endpoints", out.onEvent, endpointsEqual)**
	//================out.onEvent()
	  // processEndpointEvent triggers the config update.
		processEndpointEvent()-->
			updateEDS(c, epc, ep, event)-->
				c.xdsUpdater.EDSUpdate(c.clusterID, string(host), ns, endpoints)
	//================
newEndpointSliceController()
**registerHandlers(c.nodeInformer, c.queue, "Nodes", c.onNodeEvent, nil)**
//================c.onNodeEvent()
  // no nodePort gateway service found, no update
//================
**registerHandlers(c.pods.informer, c.queue, "Pods", c.pods.onEvent, nil)**
//================c.pods.onEvent()
  // add to cache if the pod is running or pending
	pc.update(ip, key)
	// find all related services and update CDS
	// handler is registerred in pilot/pkg/serviceregistry/kube/controller/controller.go:WorkloadInstanceHandler
	WorkloadInstanceHandler()
//================
```